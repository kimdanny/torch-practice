{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fixed-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "magnetic-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data generation\n",
    "# https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e\n",
    "np.random.seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "x = np.random.rand(100, 1)\n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1)\n",
    "\n",
    "# Shuffles the indices\n",
    "idx = np.arange(100)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:80]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[80:]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "nonprofit-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)  # why do we call float()?\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "extreme-artist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You will find it out later about why we call float() when creating dataset\n",
    "# Both float()'ed tensor and torch's default float are float32 type.\n",
    "x_train_tensor.dtype, torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-devices",
   "metadata": {},
   "source": [
    "What if we do not call float()?  \n",
    "It's float64!! This is considered as 'Double'.  \n",
    "\n",
    "The thing is that Pytorch models that inherits nn.Module \n",
    "expects parameters of float32 by default.  \n",
    "Therefore if we put float64 into our model it will spit out an error, saying  \n",
    "**\"RuntimeError: expected scalar type Float but found Double\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "handled-donna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_no_float_call = torch.from_numpy(x_train).to(device)\n",
    "x_train_no_float_call.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-advantage",
   "metadata": {},
   "source": [
    "## Manual implementation of Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "physical-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.a + self.b * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "roman-runner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([0.2576])), ('b', tensor([1.0971]))])\n",
      "OrderedDict([('a', tensor([1.0235])), ('b', tensor([1.9690]))])\n"
     ]
    }
   ],
   "source": [
    "model = MyLinearRegression().to(device)\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 1e-1\n",
    "epochs = 1000\n",
    "\n",
    "mse_loss = nn.MSELoss(reduction='mean')\n",
    "# because in the MyLinearRegression class we have set nn.Parameter()s, \n",
    "# we can call model.parmeters() as optimizer params\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    y_hat = model(x_train_tensor)\n",
    "    loss = mse_loss(y_train_tensor, y_hat)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-retro",
   "metadata": {},
   "source": [
    "## torch.nn.Linear()  --> Nested model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "twelve-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-liberia",
   "metadata": {},
   "source": [
    "### Automating training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "smooth-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_train_step_function(model, loss_function, optimizer):\n",
    "    # Build function that is to be returned and used in every epoch\n",
    "    def train_step(x, y):\n",
    "        model.train()\n",
    "        y_hat = model(x)\n",
    "        loss = loss_function(y, y_hat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "    \n",
    "    # returning a function\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "proof-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLinearModel().to(device)\n",
    "\n",
    "train_step_func = return_train_step_function(model=model, \n",
    "                                             loss_function=nn.MSELoss(reduction='mean'),\n",
    "                                             optimizer=torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "timely-bikini",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[1.9518]])), ('linear.bias', tensor([1.0323]))])\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(300):\n",
    "    loss = train_step_func(x_train_tensor, y_train_tensor)  # returns loss.item()\n",
    "    losses.append(loss)\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-marketing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "capable-activity",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "e.g. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "prostate-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "intended-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Watch out what goes into the super method!\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # Number of channels: 1 -> 6 -> 16\n",
    "        \n",
    "        # FC layers\n",
    "        # from Conv2d layers, we have 16 channels of 6x6 image\n",
    "        # we will flatten that later in forward() and connect to 120 Nodes\n",
    "        self.fc1 = nn.Linear(16*6*6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2,2) window on ReLu'ed conv1\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        # if window size is square, can provide one number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # Flatten the matirx to vector\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # All dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "blocked-loading",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NeuralNet()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-concord",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "improving-electronics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "elder-asian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([6, 1, 3, 3])\n",
      "1 torch.Size([6])\n",
      "2 torch.Size([16, 6, 3, 3])\n",
      "3 torch.Size([16])\n",
      "4 torch.Size([120, 576])\n",
      "5 torch.Size([120])\n",
      "6 torch.Size([84, 120])\n",
      "7 torch.Size([84])\n",
      "8 torch.Size([10, 84])\n",
      "9 torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(params)):\n",
    "    print(i, params[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "historic-perth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.1101, -0.0640,  0.0269],\n",
       "          [-0.1618, -0.1212, -0.2035],\n",
       "          [-0.1999,  0.1351, -0.1900]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0920,  0.0144, -0.0516],\n",
       "          [-0.1780, -0.2131, -0.0819],\n",
       "          [-0.2825, -0.1357, -0.2350]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1570, -0.1577, -0.2885],\n",
       "          [ 0.3063,  0.2045,  0.1341],\n",
       "          [ 0.0466, -0.2474,  0.2811]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3035, -0.0514,  0.0914],\n",
       "          [-0.3250,  0.0667,  0.1270],\n",
       "          [-0.2774, -0.1120,  0.1409]]],\n",
       "\n",
       "\n",
       "        [[[-0.1817, -0.1134, -0.2313],\n",
       "          [ 0.2806,  0.2203,  0.1618],\n",
       "          [ 0.2273,  0.2513,  0.1589]]],\n",
       "\n",
       "\n",
       "        [[[-0.2056,  0.3156,  0.0334],\n",
       "          [ 0.2586,  0.3097, -0.2287],\n",
       "          [ 0.2469, -0.2683, -0.3172]]]], requires_grad=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0]  # conv1's weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "finnish-brooklyn",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1241,  0.1750, -0.3206,  0.1212, -0.0399, -0.3041],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-backing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "proper-mixture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0259,  0.0583, -0.1105,  0.0673,  0.0034,  0.0630, -0.1216,  0.0342,\n",
       "         -0.0689, -0.1136]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try random 32x32 input\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "according-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "moderate-fellow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0335, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # dummy target\n",
    "target = target.view(1, -1) # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "absolute-species",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MseLossBackward at 0x7ff2fc2303a0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn  # MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "brazilian-assembly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddmmBackward at 0x7ff2fc230d00>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn.next_functions[0][0]  # Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "super-insulation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AccumulateGrad at 0x7ff2fd365d30>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn.next_functions[0][0].next_functions[0][0]  # ReLu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-source",
   "metadata": {},
   "source": [
    "### Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "comparative-fusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "Parameter containing:\n",
      "tensor([ 0.1241,  0.1750, -0.3206,  0.1212, -0.0399, -0.3041],\n",
      "       requires_grad=True)\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()  # zeros the gradient buffers of all params\n",
    "print(net.conv1)\n",
    "print(net.conv1.bias)\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "hindu-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "entire-machinery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "Parameter containing:\n",
      "tensor([ 0.1241,  0.1750, -0.3206,  0.1212, -0.0399, -0.3041],\n",
      "       requires_grad=True)\n",
      "tensor([-0.0077,  0.0041, -0.0051,  0.0052,  0.0027,  0.0074])\n"
     ]
    }
   ],
   "source": [
    "print(net.conv1)\n",
    "print(net.conv1.bias)\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-reference",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-guess",
   "metadata": {},
   "source": [
    "### Weight Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cloudy-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD Manual implementation\n",
    "# weight = weight - lr * gradient\n",
    "lr = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "suburban-palestine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "theoretical-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in training loop\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    output = net(input)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-schedule",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-grill",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
